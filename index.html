<!DOCTYPE html>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
  <title>Marie Pelissier-Combescure</title>
  <meta name="author" content="Marie Pelissier-Combescure" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <link rel="shortcut icon" href="graphics/favicon.ico" type="image/x-icon" />
  <link rel="stylesheet" type="text/css" href="stylesheet.css" />
  <link  rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" />
  <link rel="stylesheet" href="graphics/icons/academicons-1.9.4/css/academicons.min.css"/>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

</head>

<body>
  <table style="width: 100%; max-width: 800px; border: 0px; border-spacing: 0px; border-collapse: separate; margin-right: auto; margin-left: auto;">
    <tbody>
      <tr style="padding: 0px">
        <td style="padding: 0px">
          <table style="width: 100%; border: 0px; border-spacing: 0px; border-collapse: separate; margin-right: auto; margin-left: auto;">
            <tbody>
              <tr style="padding: 0px">
                <td style="padding: 2.5%; width: 63%; vertical-align: middle">
                  <p class="name" style="text-align: center">
                    Marie Pelissier-Combescure
                  </p>
                  <p>
                    I am an engineer and a doctor (PhD) from IRIT in Toulouse, France, specializing in computer vision, graphics, and artificial intelligence. 
                    <p></p>
                    For the past two years, I have been a Temporary Teaching and Research Assistant (ATER) at Toulouse INP, France."
                    <p></p>
                    My <a href="https://theses.fr/2024TLSEP063">thesis</a> focused on selecting the optimal viewpoint of a 3D object to maximize visual information and identify the most relevant view — the one that best highlights essential features for object recognition. I approached this problem from two angles: first, through multimodal (2D/3D) processing based on analyzing views from images of the 3D object; and second, by working directly on 3D meshes. Now, my research aimed at improving 3D classification performance using multi-view and multimodal neural networks adapted to 3D data, integrating the evaluation of viewpoint relevance.
                    <p></p>
                    My supervisors were <a href="https://scholar.google.com/citations?user=H8QDhhAAAAAJ&hl=fr">Géraldine Morin</a> and <a href="https://scholar.google.com/citations?user=yquWZwoAAAAJ&hl=fr">Sylvie Chambon</a>.
                  </p>
                  <p style="text-align: center; font-size: 1.5em;">
                    <a href="mailto:marie.pelissier.comb@gmail.com" title="Email">
                      <i class="fas fa-envelope" style="font-size: 2em;"></i>
                    </a>
                    &nbsp;&nbsp;
                    <a href="https://scholar.google.com/citations?user=I-oiFiQAAAAJ&hl=fr" title="Google Scholar" target="_blank" rel="noopener noreferrer">
                      <!-- <i class="fas fa-graduation-cap" style="font-size: 2em;"></i> -->
                       <i class="ai ai-google-scholar" style="font-size: 2em;"></i>
                    </a>
                    &nbsp;&nbsp;
                    <a href="https://hal.science/search/index?q=pelissier-combescure" title="HAL" target="_blank" rel="noopener noreferrer">
                       <i class="ai ai-hal" style="font-size: 2em;"></i>
                    </a>
                    &nbsp;&nbsp;
                    <a href="pdf/CV_Pelissier_Marie-EN.pdf"  target="_blank">
                      <i class="ai ai-cv" style="font-size: 2.2em;"></i>En
                    </a>
                    &nbsp;&nbsp;
                    <a href="pdf/CV_Pelissier_Marie.pdf"  target="_blank">
                      <i class="ai ai-cv" style="font-size: 2.2em;"></i>Fr
                    </a>
                  </p>
                </td>
                <td style="padding: 2.5%; width: 40%; max-width: 40%">
                  <a> <img class="profile-pic" src="graphics/images/Photo-profil-updated.jpg" width="250" height="250" /> </a>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width: 100%; border: 0px; border-spacing: 0px; border-collapse: separate; margin-right: auto; margin-left: auto;">
            <tbody>
              <tr>
                <td style="padding: 20px; width: 100%; vertical-align: middle">
                  <h2>Research</h2>
                </td>
              </tr>
            </tbody>
          </table>

          <table style="width: 100%; border: 0px; border-spacing: 0px; border-collapse: separate; margin-right: auto; margin-left: auto;">
            <tbody>
<!-- -------------------------------------------------------------- -->
              <!-- 2024 Publications Header -->
              <tr>
                <td colspan="2">
                  <h2 style="padding-left: 20px; border-bottom: 1px solid #ddd; color: #4b6cb7;">2024</h2>
                </td>
              </tr>
              <!-- THESE 2024 -->
              <tr class="paper-entry">
              <td style="padding: 20px; width: 25%; vertical-align: middle">
                <!--  <div style="position: relative; width: 200px; height: 200px;"> -->
                  <!-- GIF in top-left -->
                  <!-- <img src="graphics/visapp/gif_gorgoile_blanc_60ms_cropped.gif" -->
                      <!-- style="position: absolute; top: 0; left: 0; width: 80px; height: 80px; border-radius: 8px;" /> -->
                  <!-- PNG in bottom-right -->
                  <!-- <img src="graphics/images/scia.png" -->
                      <!-- style="position: absolute; bottom: 0; right: 0; width: 130px; height: 130px; border-radius: 8px;" /> -->
                <!-- </div>  -->
                <div class="gif-container">
                  <img class="gif-img" src="graphics/gif_these_marie_60.gif" width="250" />
                </div>
              </td>
                <td style="padding: 20px; width: 75%; vertical-align: middle">
                  <span class="papertitle">Analyse de contenus visuels en 2D et en 3D : évaluation de la pertinence d'un point de vue d'un objet 3D</span>
                  <br />
                  <strong>Marie Pelissier-Combescure</strong>
                  <br />
                  <em>Thesis defense: June 24, 2024</em>, Toulouse, France
                  <br />
                  <p></p>
                  <a href="https://theses.fr/2024TLSEP063" target="_blank" rel="noopener noreferrer" style="text-decoration: none">
                      <i class="fas fa-file-pdf" style="font-size: 1.5em; vertical-align: middle; margin-right: 0.3em;"></i>Manuscrit</a>
                  <p></p>
                  <p>
                  This thesis aims to automatically select the most relevant 2D viewpoint for a given 3D object to facilitate identification and understanding. It quantifies viewpoint relevance by extracting essential object characteristics from available viewpoints. The research explores two main axes: evaluating the relevance of fixed viewpoints from textured images using geometric attributes and photographic recommendations, and determining the most representative viewpoint of an untextured 3D mesh by analyzing visible surface and intrinsic saliency based on viewing angle.
                  <br /></p>
                </td>
              </tr>

              <!-- VISAPP 2024 -->
              <tr class="paper-entry">
                <td style="padding: 20px; width: 25%; vertical-align: middle">
                  <div class="gif-container">
                    <img class="gif-img" src="graphics/visapp/gif_gorgoile_blanc_60ms_cropped.gif" width="160" />
                  </div>
                </td>
                <td style="padding: 20px; width: 75%; vertical-align: middle">
                  <span class="papertitle">Most Relevant Viewpoint of an Object: a View-Dependent 3D Saliency Approach</span>
                  <br />
                  <strong>Marie Pelissier-Combescure</strong>,
                  <a href="https://scholar.google.com/citations?user=yquWZwoAAAAJ&hl=fr">Sylvie Chambon</a>,
                  <a href="https://scholar.google.com/citations?user=H8QDhhAAAAAJ&hl=fr">Géraldine Morin</a>                  
                  <br />
                  <em>VISAPP International Conference on Computer Vision Theory and Applications</em>, 2024, Rome, Italy
                  <br />
                  <p></p>
                  <a href="https://hal.science/hal-05007327" target="_blank" rel="noopener noreferrer" style="text-decoration: none">
                      <!-- <i class="ai ai-hal" style="font-size: 1.5em; vertical-align: middle; margin-right: 0.3em;"></i>HAL</a> -->
                       <i class="ai ai-hal" style="font-size: 1.5em; vertical-align: middle; margin-right: 0.3em;"></i>HAL</a>
                  <p></p>
                  <p>
                  This paper introduces a new method for selecting the most relevant viewpoint of a 3D object, aiming to showcase the object effectively rather than focusing on aesthetics. The approach combines visibility and view-dependent saliency, incorporating factors like visible surface size and the saliency of visible vertices. A key contribution is an extensive evaluation protocol that includes a user study with over 200 participants and an analysis of image datasets to align with human preferences. The method outperforms existing techniques in selecting viewpoints that are most similar to human choices, even offering informative views where human biases might differ.
                  <br />
                  <a href="publications/3D-Vast-ICCV2025.html" style="color:#4b6cb7;">[Project Page is coming !]</a>
                  </p>
                </td>
              </tr>
<!-- --------------------------------------------------------------- -->
               <!-- 2023 Publications Header -->
              <tr>
                <td colspan="2">
                  <h2 style="padding-left: 20px; border-bottom: 1px solid #ddd; color: #4b6cb7;">2023</h2>
                </td>
              </tr>

                <!-- JFIG 2023 -->
              <tr class="paper-entry">
                <td style="padding: 20px; width: 25%; vertical-align: middle">
                  <div style="width: 160px; height: 160px; display: flex; justify-content: center; align-items: center;">
                    <img src="graphics/visapp/sphere_cameras.png" style="width: 170; height: 150px; border-radius: 8px;" />
                  </div>
                </td>
                <td style="padding: 20px; width: 75%; vertical-align: middle">
                  <span class="papertitle">Most Relevant Viewpoint of an Object: a View-Dependent 3D Saliency Approach</span>
                  <br />
                  <strong>Marie Pelissier-Combescure</strong>,
                  <a href="https://scholar.google.com/citations?user=yquWZwoAAAAJ&hl=fr">Sylvie Chambon</a>,
                  <a href="https://scholar.google.com/citations?user=H8QDhhAAAAAJ&hl=fr">Géraldine Morin</a>
                  <br />
                  <em>J.FIG Les journées Françaises de l'Informatique Graphique</em>, 2023, Montpellier, France
                  <br />
                  <p></p>
                  <a href="https://hal.science/hal-05007367" target="_blank" rel="noopener noreferrer" style="text-decoration: none">
                      <i class="ai ai-hal" style="font-size: 1.5em; vertical-align: middle; margin-right: 0.3em;"></i>HAL</a>
                  <p></p>
                  <p>                  
                  This paper presents a method for automatically selecting the most relevant 2D viewpoint of a 3D object by quantifying visible "essential object characteristics" rather than aesthetics. This is a preliminary result of the main paper published at <a href="publications/3D-Vast-ICCV2025.html" style="color:#4b6cb7;">VISAPP 2024</a>, mentioned above.
                  <br /></p>
                </td>
              </tr>

              <!-- SCIA 2023 -->
              <tr class="paper-entry">
                <td style="padding: 20px; width: 25%; vertical-align: middle">
                  <div style="width: 160px; height: 160px; display: flex; justify-content: center; align-items: center;">
                    <img src="graphics/images/scia.png" style="width: 170px; height: 170px; border-radius: 8px;" />
                  </div>
                </td>
                <td style="padding: 20px; width: 75%; vertical-align: middle">
                  <span class="papertitle">To quantify an image relevance relative to a target 3D object</span>
                  <br />
                  <strong>Marie Pelissier-Combescure</strong>,
                  <a href="https://scholar.google.com/citations?user=H8QDhhAAAAAJ&hl=fr">Géraldine Morin</a>,
                  <a href="https://scholar.google.com/citations?user=yquWZwoAAAAJ&hl=fr">Sylvie Chambon</a>                  
                  <br />
                  <em>SCIA The Scandinavian Conference on Image Analysis</em>, 2023, Levi, Finland
                  <br />
                  <p></p>
                  <a href="https://hal.science/hal-05007308" target="_blank" rel="noopener noreferrer" style="text-decoration: none">
                      <i class="ai ai-hal" style="font-size: 1.5em; vertical-align: middle; margin-right: 0.3em;"></i>HAL</a>
                  <p></p>
                  <p>                  
                  This paper introduces methods to quantify the relevance of 2D images in showcasing a 3D object, aiming to identify views that present essential characteristics regardless of texture. It proposes a deterministic relevance score based on geometric attributes and photographic recommendations, utilizing curvilinear saliency to extract features and filter irrelevant information. The paper also explores a learning-based approach using confidence scores from neural networks. Evaluation is performed using objective image rankings based on simulated degradations, demonstrating the deterministic method's efficiency and robustness, and providing insights into the behavior of learning-based methods.
                  <br /><a href="publications/3D-Vast-ICCV2025.html" style="color:#4b6cb7;">[Project Page is coming !]</a>
                </p>
                </td>
              </tr>

              <!-- JNIM 2023 -->
              <tr class="paper-entry">
                <td style="padding: 20px; width: 25%; vertical-align: middle">
                  <div style="width: 160px; height: 160px; display: flex; justify-content: center; align-items: center;">
                    <img src="graphics/images/poster_jnim.png" style="width: 150px; height: auto; border-radius: 8px;" />
                  </div>
                </td>
                <td style="padding: 20px; width: 75%; vertical-align: middle">
                  <span class="papertitle">To quantify an image relevance relative to a target 3D object</span>
                  <br />
                  <strong>Marie Pelissier-Combescure</strong>,
                  <a href="https://scholar.google.com/citations?user=H8QDhhAAAAAJ&hl=fr">Géraldine Morin</a>,
                  <a href="https://scholar.google.com/citations?user=yquWZwoAAAAJ&hl=fr">Sylvie Chambon</a>                  
                  <br />
                  <em>JNIM Journées nationales du GDR IM</em>, 2023, Paris, France
                  <br />
                  <p></p>
                  <a href="https://hal.science/hal-05045686" target="_blank" rel="noopener noreferrer" style="text-decoration: none">
                      <i class="ai ai-hal" style="font-size: 1.5em; vertical-align: middle; margin-right: 0.3em;"></i>HAL</a>
                    <a href="https://hal.science/hal-05045686" target="_blank" rel="noopener noreferrer" style="text-decoration: none">/Poster</a>
                  <p></p>
                  <p>
                  This poster, presenting results that earned the best contribution award at GTMG 2022, details a method for quantifying the visual relevance of a 2D image in showcasing a 3D object.
                  <p></p>
                  <p></p>
                  <br /></p>
                </td>
              </tr>

<!-- --------------------------------------------------------------- -->
               <!-- 2022 Publications Header -->
              <tr>
                <td colspan="2">
                  <h2 style="padding-left: 20px; border-bottom: 1px solid #ddd; color: #4b6cb7;">2022</h2>
                </td>
              </tr>

                <!-- RFIAP 2022 -->
              <tr class="paper-entry">
                <td style="padding: 20px; width: 25%; vertical-align: middle">
                  <div style="width: 160px; height: 160px; display: flex; justify-content: center; align-items: center;">
                    <img src="graphics/images/classement_image_score.png" style="width: 200px; height: auto; border-radius: 8px;" />
                  </div>
                </td>
                <td style="padding: 20px; width: 75%; vertical-align: middle">
                  <span class="papertitle">Quelle image met le mieux en valeur un modèle 3D ?</span>
                  <br />
                  <strong>Marie Pelissier-Combescure</strong>,
                  <a href="https://scholar.google.com/citations?user=H8QDhhAAAAAJ&hl=fr">Géraldine Morin</a>
                  <a href="https://scholar.google.com/citations?user=yquWZwoAAAAJ&hl=fr">Sylvie Chambon</a>
                  <br />
                  <em>RFIAP Congrès Reconnaissance des Formes, Image, Apprentissage et Perception </em>, 2022, Vannes, France
                  <br />
                  <p></p>
                  <a href="https://hal.science/hal-03715237" target="_blank" rel="noopener noreferrer" style="text-decoration: none">
                      <i class="ai ai-hal" style="font-size: 1.5em; vertical-align: middle; margin-right: 0.3em;"></i>HAL</a>
                  <p></p>
                  <p>
                  This paper proposes a method to quantify how well a 2D image showcases a 3D object, ranking images based on a "relevance score" derived from object dominance, size, and curvilinear saliency information. It also compares this approach to confidence scores from neural networks. Validated using systematically degraded images, the deterministic method proves effective and robust, outperforming most deep learning models. These works have been accepted at <a href="publications/3D-Vast-ICCV2025.html" style="color:#4b6cb7;">SCIA 2023</a>.
                  <br /></p>
                </td>
              </tr>

                <!-- GTMG 2022 -->
              <tr class="paper-entry">
                <td style="padding: 20px; width: 25%; vertical-align: middle">
                  <div style="width: 160px; height: 160px; display: flex; justify-content: center; align-items: center;">
                    <img src="graphics/images/gtmg.png" style="width: 200px; height: auto; border-radius: 8px;" />
                  </div>
                </td>
                <td style="padding: 20px; width: 75%; vertical-align: middle">
                  <span class="papertitle">Représentation d'un objet 3D dans une image 2D : mesure de la qualité par apprentissage profond et par une approche déterministe</span>
                  <br />
                  <strong>Marie Pelissier-Combescure</strong>,
                  <a href="https://scholar.google.com/citations?user=H8QDhhAAAAAJ&hl=fr">Géraldine Morin</a>
                  <a href="https://scholar.google.com/citations?user=yquWZwoAAAAJ&hl=fr">Sylvie Chambon</a>
                  <br />
                  <em> GTMG Groupe de Travail en Modélisation Géométrique</em>, 2022, Dijon, France
                  <br />
                  <p></p>
                  <a href="https://hal.science/hal-05007391" target="_blank" rel="noopener noreferrer" style="text-decoration: none">
                      <i class="ai ai-hal" style="font-size: 1.5em; vertical-align: middle; margin-right: 0.3em;"></i>HAL</a>
                  <p></p>
                  <p>
                  This work is a preliminary result of the main paper published at <a href="publications/3D-Vast-ICCV2025.html" style="color:#4b6cb7;">SCIA 2023</a>, which is mentioned above.
                  <p></p>
                  🏅​ Award for best contribution
                  <br /></p>
                </td>
              </tr>

<!-- --------------------------------------------------------------- -->
               <!-- 2021 Publications Header -->
              <tr>
                <td colspan="2">
                  <h2 style="padding-left: 20px; border-bottom: 1px solid #ddd; color: #4b6cb7;">2021</h2>
                </td>
              </tr>
              <!-- ORASIS 2021 -->
              <tr class="paper-entry">
                <td style="padding: 20px; width: 25%; vertical-align: middle">
                  <div style="width: 160px; height: 160px; display: flex; justify-content: center; align-items: center;">
                    <img src="graphics/images/orasis.png" style="width: 150px; height: 150px; border-radius: 8px;" />
                  </div>
                </td>
                <td style="padding: 20px; width: 75%; vertical-align: middle">
                  <span class="papertitle">Extraction et comparaison d'information saillante : Pose favorable et image 2D révélatrice d'un objet 3D</span>
                  <br />
                  <strong>Marie Pelissier-Combescure</strong>,
                  <a href="https://scholar.google.com/citations?user=H8QDhhAAAAAJ&hl=fr">Géraldine Morin</a>
                  <a href="https://scholar.google.com/citations?user=yquWZwoAAAAJ&hl=fr">Sylvie Chambon</a>
                  <br />
                  <em> ORASIS Journées francophones des jeunes chercheurs en vision par ordinateur</em>, 2021, Revel, France
                  <br />
                  <p></p>
                  <a href="https://hal.science/hal-03339730" target="_blank" rel="noopener noreferrer" style="text-decoration: none">
                      <i class="ai ai-hal" style="font-size: 1.5em; vertical-align: middle; margin-right: 0.3em;"></i>HAL</a>
                  <p></p>
                  <p>                  
                   This paper proposes methods to quantify the essential 3D object information present in a 2D image, distinguishing between the "favorability" of an object's pose and the "relevance" of the 2D image itself. The core idea is to extract and match salient features from both 2D images and their corresponding 3D models using a curvilinear saliency detector, which ensures high repeatability between modalities. This allows for ranking poses and images based on the amount of essential information they contain about the object. The paper presents initial results that are encouraging, although it identifies areas for improvement such as handling occlusions and ensuring all salient points in depth maps are detected.
                  <br /></p>
                </td>
              </tr>      
            </tbody>
          </table>

<!-- --------------------------------------------------------------- -->     
<!-- ---------------- EDUCATION ------------------------ -->     
            <table style="width: 100%; border: 0px; border-spacing: 0px; border-collapse: separate; margin-right: auto; margin-left: auto;">
            <tbody>
              <tr>
                <td style="padding: 20px; width: 100%; vertical-align: middle">
                  <h2>Education</h2>
                </td>
              </tr>
            </tbody>
          </table>
          <table style="width: 100%; border: 0px; border-spacing: 0px; border-collapse: separate; margin-right: auto; margin-left: auto;">
            <tbody>
<!-- -------------------------------------------------------------- -->
              <!-- 2024  Header -->
              <tr>
                <td colspan="2">
                  <h2 style="padding-left: 20px; border-bottom: 1px solid #ddd; color: #4b6cb7;"></h2>
                </td>
              </tr>
              <tr>
                <td style="padding: 10px; width: 100%; vertical-align: middle;">
                  <ul style="list-style-type:none; padding-left: 0;">
                    <li>
                      <strong>PhD in Computer Science</strong>, IRIT at Toulouse, France — 2020 - 2024<br />
                      Thesis: <a href="https://theses.fr/2024TLSEP063"><em>Analyse de contenus visuels en 2D et en 3D : évaluation de la pertinence d'un point de vue d'un objet 3D</em></a> <br />
                      Supervisors: <a href="https://scholar.google.com/citations?user=H8QDhhAAAAAJ&hl=fr">Géraldine Morin</a>,
                      <a href="https://scholar.google.com/citations?user=yquWZwoAAAAJ&hl=fr">Sylvie Chambon</a>
                      <br />
                      <em>Thesis defense: June 24, 2024</em>, Toulouse
               

                    </li>
                    <li style="margin-top: 10px;">
                      <strong>Engineering Degree</strong>, ENSEEIHT at Toulouse, France — 2017 - 2020<br />
                      Specialization in computer vision, graphics, and AI
                    </li>
                  </ul>
                </td>
              </tr>
            </tbody>
          </table>
<!-- -------------------------------------------------------------- -->
<!-- -------------------------------------------------------------- -->
          <table style="width: 100%; border: 0px; border-spacing: 0px; border-collapse: separate; margin-right: auto; margin-left: auto;">
            <tbody>
              <tr>
                <td style="padding: 0px">
                  <br />
                  <p style="text-align: right; font-size: small">
                    This website is inspired from Jon Barron's. Last updated July 2025.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>

        </td>
      </tr>
    </tbody>
  </table>
</body>

</html>
